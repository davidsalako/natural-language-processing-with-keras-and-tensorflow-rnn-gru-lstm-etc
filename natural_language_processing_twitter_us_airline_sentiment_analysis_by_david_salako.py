# -*- coding: utf-8 -*-
"""Natural_Language_Processing_Twitter_US_Airline_Sentiment_Analysis_By_David_Salako.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1GY-MARKGZSOFYCxEdsJrdR5Ex2EVVz

# **Natural Language Processing - Twitter US Airline Sentiment Analysis.  - By David Salako.**

<br>

## **Background and Context.**

Twitter possesses 330 million monthly active users, which allows businesses to reach a broad population and connect with customers without intermediaries. On the other hand, there is so much information that it is difficult for brands to quickly detect negative social mentions that could harm their business.

Sentiment analysis/classification, which involves monitoring emotions in conversations on social media platforms, has become a key strategy in social media marketing.


Listening to how customers feel about the product/service on Twitter allows companies to understand their audience, keep on top of what is being said about their brand and their competitors, and discover new trends in the industry.

<br>

## **Objective.**
To implement the techniques learned as a part of the course with the following learning outcomes:

    * Basic understanding of text pre-processing.
    * What to do after text pre-processing
    * Bag of words
    * Tf-idf
    * Build the classification model.
    * Evaluate the Model

<br>

## **Dataset.**

A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as "late flight" or "rude service").

<br>

The dataset has the following columns:

    * tweet_id                                                           
    * airline_sentiment                                               
    * airline_sentiment_confidence                               
    * negativereason                                                   
    * negativereason_confidence
    * airline                                    
    * airline_sentiment_gold                                              
    * name     
    * negativereason_gold 
    * retweet_count
    * text
    * tweet_coord
    * tweet_created
    * tweet_location 
    * user_timezone

There are 14,640 rows and 15 columns.

## **Importing the libraries.**
"""

# install and import necessary libraries.

!pip install contractions

import re, string, unicodedata                          # Import Regex, string and unicodedata.
import contractions                                     # Import contractions library.
from bs4 import BeautifulSoup                           # Import BeautifulSoup.

import numpy as np                                      # Import numpy.
import pandas as pd                                     # Import pandas.
import nltk                                             # Import Natural Language Tool-Kit.

# Download Stopwords.
nltk.download('punkt')
nltk.download('wordnet')

from nltk.corpus import stopwords                       # Import stopwords.
from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.
from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.
import matplotlib.pyplot as plt                         
import seaborn as sns
import matplotlib.pyplot as plt


# Ignore the warnings
import warnings
warnings.filterwarnings("ignore")

# Loading the data from a .csv file into a pandas dataframe.

df_Tweets_Orig = pd.read_csv("Tweets.csv")
df_Tweets_Orig.head()

# Make a copy of the original dataframe to work on.

df_Tweets = df_Tweets_Orig.copy()

# View a random sampling of the rows of records.

df_Tweets.sample(n=75, random_state=1)

"""## **Exploratory Data Analysis (EDA).**"""

df_Tweets.shape    # Print the shape of the dataset.

"""**Observation:**
<br>
The dataset has 15 columns and 14640 rows of data. 
"""

df_Tweets.info() # Information of all columns in the dataframe.

"""**Observation:**
<br>
There are lots of nulls present in the attributes:
* negativereason
* negativereason_confidence
* airline_sentiment_gold
* negativereason_gold
* tweet_coord
* tweet_location
* user_timezone

"""

# View some basic statistical details like percentile, mean, std etc. of a data frame of numeric values. 

df_Tweets.describe()

df_Tweets.isnull().sum(axis=0)          # Check for NULL values.

"""**Observation:**
<br>
Null counts in the fields as stated earlier above.
"""

plt.figure(figsize=(12,7))
sns.heatmap(df_Tweets.isnull(), cmap = "Blues")                        # Visualization of missing value using heatmap.
plt.title("Missing values?", fontsize = 15)
plt.show()

"""**Observations:**
* The lines in the above heatmap help visualize the location of missing values (i.e. the missing value indexes in the dataframe).
* The following columns have so many missing values that their data is probably of little relevance: airline_sentiment_gold, negativereason_gold, and tweet_coord.
* The columns negative_reason, negative_reason_confidence, tweet_location, and user_timezone have missing values ranging in the 4000s and 5000s.
"""

# Check the missing values for all the columns.

def return_missing_values(data_frame):
    missing_values = data_frame.isnull().sum()
    missing_values = missing_values[missing_values>0]
    missing_values.sort_values(inplace=True)
    return missing_values

# Plot the count of missing values in every column.

def plot_missing_values(data_frame):
    missing_values = return_missing_values(data_frame)
    missing_values = missing_values.to_frame()
    missing_values.columns = ['count']
    missing_values.index.names = ['Name']
    missing_values['Name'] = missing_values.index
    sns.set(style='darkgrid')
    sns.barplot(x='Name', y='count', data=missing_values)
    plt.title('Bar plot for Null Values in each column')
    plt.xticks(rotation=90)
    plt.show()

# Get the count of missing values in every column of the dataframe.

return_missing_values(df_Tweets)

# Plotting the count of missing values.

plot_missing_values(df_Tweets)

"""**Observations:**

The additional table and graph further illustrate the earlier observations regarding missing values.
"""

# Get the unique values of every column.

def return_unique_values(data_frame):
    unique_dataframe = pd.DataFrame()
    unique_dataframe['Features'] = data_frame.columns
    uniques = []
    for col in data_frame.columns:
        u = data_frame[col].nunique()
        uniques.append(u)
    unique_dataframe['Uniques'] = uniques
    return unique_dataframe

# How many unique values are there in each attribute/column?

unidf = return_unique_values(df_Tweets)
print(unidf)

# Plot the count of unique values in every column.

f, ax = plt.subplots(1,1, figsize=(16,5))
sns.barplot(x=unidf['Features'], y=unidf['Uniques'], alpha=0.7)
plt.title('Bar plot for Unique Values in each column')
plt.ylabel('Unique values', fontsize=14)
plt.xlabel('Features', fontsize=14)
plt.xticks(rotation=90)
plt.show()

"""**Observations:**

A visualization of the number of unique values in each column. Tweet_id has the most because each tweet is uniquely identified which is most likely of not much value in the model building excercise.

"""

# Plot for Twitter US Airline Sentiment Labels using matplotlib.

colors = ['#ff6666', '#ffcc99', '#99ff99']

sns.set(rc={'figure.figsize':(11.7,8.27)})
plot = plt.pie(df_Tweets['airline_sentiment'].value_counts(), labels=df_Tweets['airline_sentiment'].value_counts().index, colors=colors, startangle=90,  autopct='%.2f')
centre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
plt.title('Pie plot for Twitter US Airline Sentiment Labels')
plt.axis('equal')
plt.tight_layout()
plt.show()

"""**Observations:**

* Approximately 16% of the tweets are positive in nature.
* The majority of the tweets are negative in nature at 62.7%, almost two-thirds.
* 21.2% of the tweets are deemed neutral in nature.
"""

# Count number of each type of tweet.

df_Tweets['airline_sentiment'].value_counts()

# A function to create labeled barplots.

def labeled_barplot(df_Tweets, feature,  title, pallet,perc=True, n=None):
    """
    Barplot with percentage at the top

    df_Tweets: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(df_Tweets[feature])  # length of the column
    count = df_Tweets[feature].nunique()
    if n is None:
        plt.figure(figsize=(16, 4))
    else:
        plt.figure(figsize=(16, 4))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        df_Tweets[feature],
        palette=pallet,
        order=df_Tweets[feature].value_counts().index[:20],
    )
    ax.set_title('Frequency of {} tweeting about US Airlines'.format(title))

    for p in ax.patches:
        if perc == True:
            label = "{:1.2f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

"""### **Airlines arranged by number of tweets.**"""

# Visualize the top 20 users by number of tweets.

labeled_barplot(df_Tweets, 'airline', 'Airlines','tab20')

"""**Observations:**
* United is the airline most tweeted about in February 2015 at just over 25% of the total number of tweets overall.
* There are only 6 airlines tweeted about in this dataset and for this timeframe.
* Virgin America is the least tweeted about but it is also the airline with the smallest flight network footprint compared to the other 5 large airlines.
* Southwest is the only lowcost airline among the 6 airlines in this dataset.

### **Top 20 users by number of tweets.**
"""

# Visualize the top 20 users by number of tweets.

labeled_barplot(df_Tweets, 'name', 'Names','tab20')

"""**Observations:**

* JetBlueNews (0.43%) is the name that has the highest number of tweets in this particular dataset.
* Kbosspotter (0.22%), _mhertz (0.20%), otisday (0.19%), and throthra (0.18%) come in at second, third, fourth, and fifth place respectively.
* The relatively low percentages demonstrate that there are a lot of names that tweet in this US Airline dataset.

### **Top 20 user locations based on the number of tweets.**
"""

# Visualize the top 20 sources by number of tweets.

labeled_barplot(df_Tweets, 'tweet_location','Locations', 'tab20')

"""**Observations:** 
* As we can see in the above all the locations were not in the format of city, state abbreviation.
* We take top 50 user locations with more number of tweets try to make the format into city, state abbreviation.
"""

# Take the top 500 user locations based on no of tweets.

dt = df_Tweets['tweet_location'].value_counts().reset_index()  # Get the counts of tweets which contains a fixed no of hashtags.
dt.columns = ['tweet_location', 'count'] 
dt = dt.sort_values(['count'],ascending=False)[:50] # Top 50 places.
dt.head()

# Try to make the format into city, state abbreviation for the top 50 places with more number of tweets.

city = []
state = []
for i in dt['tweet_location']:
  loc = i.split(',')
  if len(loc)>1:   # If it has more than one token.
    city.append(loc[0])
    state.append(loc[1])
  else:
    city.append('other') # If number of tokens is 1 then we keep it as other.
    state.append('other')

dt['city'] = city
dt['state'] = state
dictionary = dict(zip(dt['city'], dt['state']))  # Create a dictionary with key as city and value maps to its state.
dt.head()

# Get the final locations.

location = []

for i in dt['tweet_location']:
  loc = i.split(',')
  if len(loc)==2:  # If it has two tokens location will be same.
    location.append(loc[0]+','+loc[1])
  else:
    try:
      state = dictionary[loc[0]]  # Incase if only city is present we try to map it to the state from the above dataframe.
      location.append(loc[0]+','+state)
    except:
      location.append('other')  # If we cant find the map then we leave it.

dt["location"] = location
dt.head()

# Get the count of tweets from every place.

ds = dt.groupby(['location']).sum().sort_values(by='count', ascending=False).reset_index()

# Get the plot with no of tweets contains x number of tags.
dt = ds

fig = sns.barplot( 
    x
    =dt["count"], 
    y=dt["location"], 
    orientation='horizontal'
).set_title('Distribution of number of hashtags in tweets')

"""**Observations:**

* There are thousands of tweets (the vast majority) where the user did not complete a coherent location that is easily interpreted by the programming languages. 
* Of the readable locations, the cities of Boston, Chicago, San Francisco, Los Angeles and New York have the highest number of tweet counts. This makes sense as these are major cities with very busy airports as well as airline hubs.

### **Year Created Distribution for Tweets about US Airlines.**
"""

# Number of new users created every year.

df_Tweets['tweet_created'] = pd.to_datetime(df_Tweets['tweet_created'])  # Change the format into datetime readable.
df_Tweets['year_created'] = df_Tweets['tweet_created'].dt.year   # Get the year where user is created.
date = df_Tweets.drop_duplicates(subset='name', keep="first")  # Remove duplicates.
date = date[date['year_created']>1970]   # Consider only user created affter 1970.
date = date['year_created'].value_counts().reset_index()   # Get the count of users created every year.
date.columns = ['year', 'number']
plt.figure(figsize=(16, 4))
fig = sns.barplot( 
    x=date["year"], 
    y=date["number"], 
    orientation='vertical'
).set_title('New Users created year by year')


plt.ylabel('count', fontsize=12)
plt.xlabel('year', fontsize=12)
plt.xticks(rotation=90)
plt.show()

"""**Observation:**

As per the description of the dataset, all the tweets were generated in 2015 and specifically in the month of February.

### **Tweet distribution by day.**
"""

df = df_Tweets.sort_values(['tweet_created'])
df['day'] = df['tweet_created'].astype(str).str.split(' ', expand=True)[0]
ds = df['day'].value_counts().reset_index()  # Get the count of no of tweets for every day.
ds.columns = ['day', 'count']
ds = ds.sort_values(['day'])

ds['day'] = ds['day'].astype(str)
plt.figure(figsize=(16, 7))
fig = sns.barplot( 
    x=ds['count'], 
    y=ds["day"], 
    orientation="horizontal",
).set_title('Tweets distribution over days present in dataset')

"""**Observations:** 
* February 22nd 2015 was the date with the most generated number of tweets at 3079.
* The following date of February 23rd 2015 generated the second highest number of tweets at 3028.
* Perhaps there was a signinificant event that affected the US airline industry on these two dates?

### **Hourly Tweet distribution.**
"""

df_Tweets['tweet_created'] = pd.to_datetime(df_Tweets['tweet_created']) # Change the format into datetime readable.
df_Tweets['hour'] = df_Tweets['tweet_created'].dt.hour    # Get the hour of every tweet.
ds = df_Tweets['hour'].value_counts().reset_index()  # Get the count of no of tweets for every hour.
ds.columns = ['hour', 'count']
ds = ds.sort_values(['hour'])
ds['hour'] = 'Hour ' + ds['hour'].astype(str)
plt.figure(figsize=(16, 7))
fig = sns.barplot( 
    x=ds["hour"], 
    y=ds["count"], 
    orientation='vertical', 
).set_title('Tweets distribution over hours')
plt.xticks(rotation='vertical')

"""**Observations:**

* The majority of tweets are sent during the peak travel hours of 7AM to 7PM which is when the highest number of US airline flights criss-cross the country.
* 9AM has the overall hihest number of tweets in the dataset with two other peaks (trimodal) at 11AM and 2PM respectively.
* 1AM is the hour with the least amount of tweets.
"""

# Get the no of words in every text.

df_Tweets['word_count'] = [len(t.split()) for t in df_Tweets.text]
df_Tweets.head()

# Get the no of words in every text.

df_Tweets['word_count'] = [len(t.split()) for t in df_Tweets.text]
df_Tweets.head(100)

"""# **Data Pre-processing (with user-defined functions).**

* Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a method to resolve such issues.

**Data Pre-processing steps:**

- Remove html tags.
- Replace contractions in string. (e.g. replace I'm --> I am) and so on.\
- Remove numbers.
- Remove the URL's.
- Remove the mentions in the tweets('@').
- Tokenization.
- To remove Stopwords.
- Remove the punctuations.
- Remove the non-ASCII characters.
- Remove the hashtags.
- Lemmatized data.
- We have used the NLTK library to tokenize words, remove stopwords and lemmatize the remaining words.
"""

df_Tweets = df_Tweets[["text","airline_sentiment"]]

pd.set_option('display.max_colwidth', None) # Display full dataframe information (Non-truncated text column.)

df_Tweets.head(100)

# Remove the html tags.

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")                    
    return soup.get_text()

# Expand the contractions.

def replace_contractions(text):
    """Replace contractions in string of text"""
    return contractions.fix(text)

# Remove the numericals present in the text.

def remove_numbers(text):
  text = re.sub(r'\d+', '', text)
  return text

# Remove the url's present in the text.

def remove_url(text): 
    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)
    return text

# Remove the mentions in the tweets.

def remove_mention(text):
    text = re.sub(r'@\w+','',text)
    return text

def clean_text(text):
    text = strip_html(text)
    text = replace_contractions(text)
    text = remove_numbers(text)
    text = remove_url(text)
    text = remove_mention(text)
    return text
    
df_Tweets['text'] = df_Tweets['text'].apply(lambda x: clean_text(x))
df_Tweets.head(100)

df_Tweets['text'] = df_Tweets.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) # Tokenization of data
df_Tweets.head(100)

import nltk
nltk.download('stopwords')

stopwords = stopwords.words('english')
stopwords = list(set(stopwords)) 
lemmatizer = WordNetLemmatizer()

# Remove the non-ASCII characters.

def remove_non_ascii(words):
    """Remove non-ASCII characters from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

# Convert all characters to lowercase.

def to_lowercase(words):
    """Convert all characters to lowercase from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words


# Remove the hashtags.

def remove_hash(text):
   """Remove hashtags from list of tokenized words"""
   new_words = []
   for word in words:
     new_word = re.sub(r'#\w+','',word)
     if new_word != '':
       new_words.append(new_word)
   return new_words

# Remove the punctuations.

def remove_punctuation(words):
    """Remove punctuation from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', '', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words

# Remove the stop words.

def remove_stopwords(words):
    """Remove stop words from list of tokenized words"""
    new_words = []
    for word in words:
        if word not in stopwords:
            new_words.append(word)
    return new_words

# Lemmatize the words.

def lemmatize_list(words):
    new_words = []
    for word in words:
      new_words.append(lemmatizer.lemmatize(word, pos='v'))
    return new_words

def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = remove_stopwords(words)
    words = lemmatize_list(words)
    return ' '.join(words)

df_Tweets['text'] = df_Tweets.apply(lambda row: normalize(row['text']), axis=1)

df_Tweets.head(100)

"""**Observations:**
* The text in row 1 is categorized as a positive sentiment when in actual fact it is a negative tweet. The word "tacky" which can have more than one meaning is not being interpreted as a negative word in this context.

**Creating a Word Cloud for all the Tweets.**
"""

# Install the WordCloud library.

!pip install wordcloud

# Importing all necessary modules. 

from wordcloud import WordCloud
from wordcloud import STOPWORDS
import matplotlib.pyplot as plt
 
stopword_list = set(STOPWORDS) 

word_lists = df_Tweets['text']
unique_str  = ' '.join(word_lists)

# Generate_wordcloud(unique_str).

word_cloud = WordCloud(width = 3000, height = 2500, 
                       background_color ='white', 
                       stopwords = stopword_list, 
                       min_font_size = 10).generate(unique_str) 

# Visualize the WordCloud Plot.

# Set wordcloud figure size.

plt.figure(1,figsize=(12, 12)) 

# Show image.

plt.imshow(word_cloud) 

# Remove Axis.

plt.axis("off")  

# Show plot.

plt.show()

"""**Observations:**

Words like "flight", "thank", "help", "customer service", "bag", "time", "need", "make", and "fly" are the most frequently used in the overall dataset.

**Word Cloud for Negative Tweets.**
"""

# Create a dataset of the negative tweets.

negative_tweets=df_Tweets[df_Tweets['airline_sentiment']=='negative']
word_lists_neg = ' '.join(negative_tweets['text'])

# Set the display parameters for the word cloud.

wordcloud = WordCloud(stopwords=stopword_list,
                      background_color='red',
                      width=3000,
                      height=2500
                     ).generate(word_lists_neg)

# Display the word cloud.

plt.figure(1,figsize=(12, 12))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""**Observations:**

Amongst the negative tweets, the most frequent words are "flight", "time", "help", "bag", "customer service", "go", "cancel", "flightled", "delay" etc.

**Word Cloud for Positive Tweets.**
"""

# Create a dataset of the positive tweets.

positive_tweets=df_Tweets[df_Tweets['airline_sentiment']=='positive']
word_lists_pos = ' '.join(positive_tweets['text'])

# Set the display parameters for the word cloud.

wordcloud = WordCloud(stopwords=stopword_list,
                      background_color='green',
                      width=3000,
                      height=2500
                     ).generate(word_lists_pos)

# Display the word cloud.

plt.figure(1,figsize=(12, 12))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""**Observations:**

The positive tweets have frequent words apearing such as "flight", "great", "thank", "love", "awesome", "appreciate", "customer service", "fly", "amaze" etc.

**Word Cloud for Neutral Tweets.**
"""

# Create a dataset of the neutral tweets.

neutral_tweets=df_Tweets[df_Tweets['airline_sentiment']=='neutral']
word_lists_neu = ' '.join(neutral_tweets['text'])

# Set the display parameters for the word cloud.

wordcloud = WordCloud(stopwords=stopword_list,
                      background_color='orange',
                      width=3000,
                      height=2500
                     ).generate(word_lists_neu)

# Display the word cloud.

plt.figure(1,figsize=(12, 12))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""**Observations:**

The neutral tweets have frequent words such as "flight", "thank", "fly", "need", "please", "go", "ticket" etc.

# **Data Pre-processing (with Keras).**
"""

# Import Keras libraries.


from keras.models import Model

from keras.layers import Input, LSTM, GRU, Dense, Embedding

from keras.preprocessing.text import Tokenizer

from sklearn.preprocessing import LabelBinarizer

from keras.layers import Activation, Dropout

from keras.models import Sequential

df_Tweets_K = df_Tweets_Orig.copy()

# Size of our dataset.

df_Tweets_K.shape

# Look at the first 25 rows

df_Tweets_K.head(25)

"""**Dividing the Keras dataset into train and test.**"""

# Before we feed our data into Keras deep learning algorithms, we divided our data into training and test sets as shown below.

X = df_Tweets_K.iloc[:, 10].values

y = df_Tweets_K.iloc[:, 1].values

# The “iloc” function takes the index that we want to filter from our dataset. 
# Since our feature set will consist of tweet text, which is the 11th column, we passed 10 to the first iloc function. 
# Since column index starts from 0, therefore the 10th index corresponds to the 11th column. 
# Similarly, for labels, we pass the 1st index.

# Now variable X contains our feature set while the variable y contains our labels or the output.

"""**Text cleaning.**"""

# Remove the html tags.

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")                    
    return soup.get_text()

# Expand the contractions.

def replace_contractions(text):
    """Replace contractions in string of text"""
    return contractions.fix(text)

# Remove the numericals present in the text.

def remove_numbers(text):
  text = re.sub(r'\d+', '', text)
  return text

# Remove the url's present in the text.

def remove_url(text): 
    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)
    return text

# Remove the mentions in the tweets.

def remove_mention(text):
    text = re.sub(r'@\w+','',text)
    return text

def clean_text(text):
    text = strip_html(text)
    text = replace_contractions(text)
    text = remove_numbers(text)
    text = remove_url(text)
    text = remove_mention(text)
    return text
    
df_Tweets_K['text'] = df_Tweets_K['text'].apply(lambda x: clean_text(x))
df_Tweets_K.head(100)

df_Tweets_K['text'] = df_Tweets_K.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) # Tokenization of the dataset.
df_Tweets_K.head(100)

# Tweets contain many special characters such as @, #, – etc. Similarly, there can be many empty spaces. 
# These special characters and empty spaces normally do not help in classification, 
# therefore we clean our text before using it for deep learning purposes.

# The following script performs text cleaning tasks.

df_Tweets_K['text'] = df_Tweets_K.apply(lambda row: normalize(row['text']), axis=1)


# The script above removes all the special characters from the tweets, 
# then removes single spaces from the beginning. 
# Then all the multiple spaces, generated as the result of removing special characters, 
# are removed. Finally, for the sake of uniformity, all the text is converted to lower case. 
# Regular expressions are used in the above script for text cleaning tasks.

# Divide our data into a training and test set.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Converting Text to Numbers:

<br>

Statistical approaches like machine learning and deep learning work with numbers. However, the data we have is in the form of text. We need to convert the textual data into the numeric form. Several approaches exist to convert text to numbers such as bag of words, TFIDF and word2vec.

<br>

To convert text to numbers, we can use the “Tokenizer” class from the “keras.preprocessing.text” library. The constructor for the “Tokenizer” class takes “num_words” as a parameter which can be used to specify the minimum threshold for the most frequently occurring words. This can be helpful since the words that occur less number of times than a certain threshold are not very helpful for classification. Next, we need to call the “fit_on_text()” method to train the tokenizer. Once we train the tokenizer, we can use it to convert text to numbers using the “text_to_matrix()” function. The “mode” parameter specifies the scheme that we want to use for the conversion. We used TFIDF scheme owing to its simplicity and efficiency. The following script converts, text to numbers. 
"""

vocab_size = 1000

tokenizer = Tokenizer(num_words=vocab_size)

tokenizer.fit_on_texts(X_train)

train_tweets = tokenizer.texts_to_matrix(X_train, mode='tfidf')

test_tweets = tokenizer.texts_to_matrix(X_test, mode='tfidf')

# Our labels are also in the form of text e.g. positive, neutral and negative. 
# We need to convert it into text as well. 
# To do so, we used the “LabelBinarizer()” from the “sklearn.preprocessing” library.

encoder = LabelBinarizer()

encoder.fit(y_train)

train_sentiment = encoder.transform(y_train)

test_sentiment = encoder.transform(y_test)

"""# **Training the Neural Network (with Keras).**

"""

# Set the neural network parameters and hyperparameters.

model = Sequential()

model.add(Dense(512, input_shape=(vocab_size,)))

model.add(Activation('relu'))

model.add(Dropout(0.3))

model.add(Dense(512))

model.add(Activation('relu'))

model.add(Dropout(0.3))

model.add(Dense(512))

model.add(Activation('relu'))

model.add(Dropout(0.3))

model.add(Dense(3))

model.add(Activation('softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',

optimizer='adam',

metrics=['accuracy'])

model_info = model.fit(train_tweets, train_sentiment,

batch_size=256,

epochs=100,

verbose=1,

validation_split=0.1)

"""**Evaluating the algorithm.**"""

# As the last step, we evaluate the performance of our algorithm on the test set using the following script:

result = model.evaluate(test_tweets, test_sentiment,

batch_size=256, verbose=1)

print('Test accuracy:', result [1])

"""**Observations:**
<br>
Not a strong model performance in validation, and test. Overfitting in training. To further improve the accuracy, we can try a different number of layers, drop out, epochs, and activation.

Harnessing the TfidVectorizer to prepare the datatset for the Naive Bayes Classifier model.
"""

# using Tfidf Vectorizer.

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features = 2000)
x_train_vec = vectorizer.fit_transform(X_train)
x_test_vec = vectorizer.transform(X_test)

"""**Using the Naive Bayes Classifier to generate an accuracy score.**"""

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
clf.fit(x_train_vec, y_train)
y_pred = clf.predict(x_test_vec)
print(clf.score(x_train_vec, y_train))

"""The score of 76.81% is not that great but falls in line with the test accouracy of the immediately preceding neural network fed by a keras generated dataset."""

# Create and write the results to a local .csv file.

np.savetxt("Predictions_twitter_sentiments.csv", y_pred, fmt="%s")

"""# **Data Pre-processing (with Tensorflow).**


"""

# Make a copy of the original dataframe to use specifically for the Tensorflow data pre-processing.

df_Tweets_TF = df_Tweets_Orig.copy()

# Look at the first 25 rows.

df_Tweets_TF.head(25)

# For sentiment analysis I am only focused on the 2 columns that contains the tweet text and sentiment label.

df_Tweets_TF[['text', 'airline_sentiment']].head(25)

# Count the numbers of each type of tweet.

df_Tweets_TF['airline_sentiment'].value_counts()

# Convert the airline_sentiment attribute into integer data format.
# Tensorflow requires the data in this format.

df_Tweets_TF ['airline_sentiment'] = df_Tweets_TF ['airline_sentiment'].replace('neutral', 1)
df_Tweets_TF ['airline_sentiment'] = df_Tweets_TF ['airline_sentiment'].replace('negative', 0)
df_Tweets_TF ['airline_sentiment'] = df_Tweets_TF ['airline_sentiment'].replace('positive', 2)

# Split the dataset into text (the independent variable) and label (the dependent variable).

X = df_Tweets_TF['text'] # data
y = df_Tweets_TF['airline_sentiment'] # labels

# Import the relevant Tensorflow libraries.

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
from tensorflow.keras.utils import to_categorical

# Convert the training data into tensors to feed into the neural network.
# Create the tokenizer class object.

t = Tokenizer()
t.fit_on_texts(X)

# How many unique words are present in the Twitter US Airline dataset?

vocab_size = len(t.word_index) + 1

# Texts are encoded into numeric inter values so the model can machine learn.

sequences = t.texts_to_sequences(X)

# Create a function to find what the longest tweet in the dataset is.

def max_tweet():
    for i in range(1, len(sequences)):
        max_length = len(sequences[0])
        if len(sequences[i]) > max_length:
            max_length = len(sequences[i])
    return max_length

tweet_num = max_tweet()
tweet_num

"""**Observation:**
<br>
The longest tweet has 30 words.
"""

# Each tweet has a different number of words, so the shorter sequences are padded with 0's.
# https://realpython.com/python-keras-text-classification/

from tensorflow.keras.preprocessing.sequence import pad_sequences
maxlen = tweet_num
padded_X = pad_sequences(sequences, padding='post', maxlen=maxlen)

# Convert the labels to a categorical numpy array.

labels = to_categorical(np.asarray(y))

# Split the dataset into train and test.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(padded_X, labels, test_size = 0.2, random_state = 0)

# Display the sizes of the train and test datasets.

print('X_train size:', X_train.shape)
print('y_train size:', y_train.shape)
print('X_test size:', X_test.shape)
print('y_test size:', y_test.shape)

"""**Word Embedding.**

In NLP, textual data must be represented in a way that computers can work with. We will focus on word embeddings which is a representation of text where similar words have a similar representation. One model of word embedding is word2vec which takes a large corpus of text and outputs a vector space where each unique word has its own corresponding vector. In this space, words with similar meanings are located close to one another.

<br>

Another popular model is the Global Vectors for Word Representation (GloVe) which is an extension of word2vec. It generally allows for better word embeddings by creating a word-context matrix. Basically, it creates a measure to indicate that certain words are more likely to be seen in context of others. For example, the word “chip” is likely to be seen in the context of “potato” but not with “cloud”. Its developers created the embeddings using English words obtained from Wikipedia and Common Crawl data.

<br>

I will use a pre-trained word embedding, because I believe GloVe generalizes well with the dataset. The embedding space created by GloVe likely contains all the words we will encounter in our tweets, so we can use these vector representations instead of creating our own from a much more limited vocabulary set.

<br>
"""

# Load the whole embedding into memory.
# GloVe is an unsupervised learning algorithm for obtaining vector representations for words. 
# Training is performed on aggregated global word-word co-occurrence statistics from a corpus, 
# and the resulting representations showcase interesting linear substructures of the word vector space.
# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/
# 100 dimensional version (embedding dimension).

embeddings_index = dict()
f = open('glove.6B.100d.txt') # the text file was downloded from the Stanford University website (https://nlp.stanford.edu/projects/glove/).
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

# GloVE requires the creation of a word embedding/word context matrix.
# Create a matrix of weights for words in the training set.
# One embedding for each word in the training set.

# Get all unique words in our training set: Tokenizer index.
# Find the corresponding weight vector in GloVe embedding.

# Define size of embedding matrix: number of unique words x embedding dim (100).

embedding_matrix = np.zeros((vocab_size, 100))

# Fill in the matrix.

for word, i in t.word_index.items():  # Dictionary.
    embedding_vector = embeddings_index.get(word) # Retreives the embedded vector of word from GloVe.
    if embedding_vector is not None:
        # add to matrix
        embedding_matrix[i] = embedding_vector # Each row of the matrix.

# Create embedding layer using embedding matrix.

from tensorflow.keras.layers import Embedding

# Input is vocab_size, output is 100.
# Weights from embedding matrix, set trainable = False.

embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix],
                           input_length = tweet_num, trainable=False)

"""# **Building the Neural Network (with Tensorflow).**"""

# Imoort the Tensorflow libraries.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import GRU
from tensorflow.keras.layers import BatchNormalization

"""**Model 1: Simple LSTM Model with regularization, increase dimensionality.**

Long Short-Term Memory (LSTM):

Simple Recurrent Neural Networks (RNNs) suffer from the vanishing gradient problem which occurs when information from earlier layers disappear as the network becomes deeper. A LSTM algorithm was created to avoid this problem by allowing the neural network to carry information across multiple time steps. This means it can save important information for later use, preventing gradients from vanishing during the process. Additionally, a LSTM cell can determine what information to remove as well. Therefore, it can learn to recognize an important input and store it for the future while removing unnecessary information.
"""

# Set up the LSTM model.

lstm_model1 = Sequential()
lstm_model1.add(embedding_layer)
lstm_model1.add(LSTM(256, 
               dropout = 0.2, 
               recurrent_dropout = 0.5))
lstm_model1.add(Dense(3, activation='softmax'))
lstm_model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
lstm_model1.summary()

# Set the hyperparameters in the LSTM model.

hist_1 = lstm_model1.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

# Train and test accuracy.

loss, accuracy = lstm_model1.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = lstm_model1.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""**Observation:**

Slight overfitting in training and 78% accuracy in testing.
"""

# Plot train/test loss and accuracy.

acc = hist_1.history['acc']
val_acc = hist_1.history['val_acc']
loss = hist_1.history['loss']
val_loss = hist_1.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then increases to an unacceptable level in validation.
"""

# Confusion matrix.

from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels

# Get predicted values
y_pred = lstm_model1.predict(X_test)  # outputs probabilities of each sentiment
# Create empty numpy array to match length of training observations
y_pred_array = np.zeros(X_test.shape[0])

# Find class with highest probability
for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # column with max probability
    y_pred_array[i] = label_predict

# convert to integers
y_pred_array = y_pred_array.astype(int)

# Convert y_test to 1d numpy array
y_test_array = np.zeros(X_test.shape[0])

# Find class with 1
for i in range(0, y_test.shape[0]):
    label_predict = np.argmax(y_test[i])
    y_test_array[i] = label_predict

y_test_array = y_test_array.astype(int)

class_names = np.array(['Negative', 'Neutral', 'Positive'])

# Create the function to plot the confusion matrix.

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:**
<br>
We see in the above confusion matrices, Model 1 did an excellent job predicting a negative label when the tweet was negative but suffered more with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance.

**Model 2: LSTM with regularization and reduced dimensionality.**
"""

lstm_model2 = Sequential()
lstm_model2.add(embedding_layer)
lstm_model2.add(LSTM(64, 
               dropout = 0.2, 
               recurrent_dropout = 0.5))
lstm_model2.add(Dense(3, activation='softmax'))
lstm_model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
lstm_model2.summary()

hist_2 = lstm_model2.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

# Train and test accuracy
loss, accuracy = lstm_model2.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = lstm_model2.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

# Plot train/test loss and accuracy
acc = hist_2.history['acc']
val_acc = hist_2.history['val_acc']
loss = hist_2.history['loss']
val_loss = hist_2.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then increases to between 60% and 70%.
"""

# Get the predicted values.

y_pred = lstm_model2.predict(X_test)  # outputs probabilities of each sentiment
# Create empty numpy array to match length of training observations
y_pred_array = np.zeros(X_test.shape[0])

# Find the class with highest probability.

for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # column with max probability
    y_pred_array[i] = label_predict

# convert to integers
y_pred_array = y_pred_array.astype(int)

np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:**
<br>
We see in the above confusion matrices, Model 2 did an excellent job predicting a negative label when the tweet was negative but did not do as well with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance. Model 2 performed better than Model 1 with the positive and neutral labels but performed approximately as well as Model 1 with the negative lables.

**Model 3: LSTM Layer Stacking.**
"""

# LSTM Model.

lstm_model3 = Sequential()
lstm_model3.add(embedding_layer)
lstm_model3.add(LSTM(256, 
               dropout = 0.2, 
               recurrent_dropout = 0.5,
                 return_sequences = True))
lstm_model3.add(LSTM(128,
                dropout = 0.2,
                recurrent_dropout = 0.5))
lstm_model3.add(Dense(3, activation='softmax'))
lstm_model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

lstm_model3.summary()

# Neural network parameters.

history_3 = lstm_model3.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

# Find train and test accuracy.

loss, accuracy = lstm_model3.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = lstm_model3.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

# Plot train/test loss and accuracy.

acc = history_3.history['acc']
val_acc = history_3.history['val_acc']
loss = history_3.history['loss']
val_loss = history_3.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then increases to over 100%. Not ideal.
"""

# Get the predicted values.

y_pred = lstm_model3.predict(X_test)  # Outputs probabilities of each sentiment.

# Create empty numpy array to match length of training observations.

y_pred_array = np.zeros(X_test.shape[0])

# Find the class with the highest probability.

for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # Column with max probability.
    y_pred_array[i] = label_predict

# Convert to integers.

y_pred_array = y_pred_array.astype(int)

np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:**

We see in the above confusion matrices, Model 3 did an excellent job predicting a negative label when the tweet was negative but did not do as well with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance. Model 3 performed better than Model 2 with the positive and neutral labels but performed approximately as well as Model 2 with the negative lables.

**Model 4: GRU Layer Stacking.**
"""

# GRU Model: 

gru_model_4 = Sequential()
gru_model_4.add(embedding_layer)
gru_model_4.add(GRU(256, 
               dropout = 0.2, 
               recurrent_dropout = 0.5,
                 return_sequences = True))
gru_model_4.add(GRU(128,
                dropout = 0.2,
                recurrent_dropout = 0.5))
gru_model_4.add(Dense(3, activation='softmax'))
gru_model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

gru_model_4.summary()

# Tune the hyperparameters.

history_4 = gru_model_4.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

loss, accuracy = gru_model_4.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = gru_model_4.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

acc = history_4.history['acc']
val_acc = history_4.history['val_acc']
loss = history_4.history['loss']
val_loss = history_4.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then increases to over 100%. Not ideal.
"""

# Get the predicted values.

y_pred = gru_model_4.predict(X_test)  # Outputs probabilities of each sentiment.

# Create empty numpy array to match length of training observations.

y_pred_array = np.zeros(X_test.shape[0])

# Find the class with highest probability.

for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # Column with max probability.
    y_pred_array[i] = label_predict

# C onvert to integers.

y_pred_array = y_pred_array.astype(int)

np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:**

We see in the above confusion matrices, Model 4 did an excellent job predicting a negative label when the tweet was negative but did not do as well with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance. Model 4 performed better than Model 3 with the neutral labels, a little worse with the positive labels, but performed approximately as well as Model 3 with the negative lables.

**Model 5: Reduced GRU with More Regularization.**
"""

# GRU Model:

gru_model_5 = Sequential()
gru_model_5.add(embedding_layer)
gru_model_5.add(GRU(64, 
               dropout = 0.3, 
               recurrent_dropout = 0.5,
                 return_sequences = True))
gru_model_5.add(GRU(32,
                dropout = 0.2,
                recurrent_dropout = 0.5))
gru_model_5.add(Dense(3, activation='softmax'))
gru_model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

gru_model_5.summary()

# Hyperparameters for the GRU Model.

history_5 = gru_model_5.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

loss, accuracy = gru_model_5.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = gru_model_5.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

acc = history_5.history['acc']
val_acc = history_5.history['val_acc']
loss = history_5.history['loss']
val_loss = history_5.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then levels out at about 50%.
"""

# Get the predicted values.

y_pred = gru_model_5.predict(X_test)  # Outputs probabilities of each sentiment.

# Create empty numpy array to match length of training observations.

y_pred_array = np.zeros(X_test.shape[0])

# Find class with highest probability.

for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # Column with max probability.
    y_pred_array[i] = label_predict

# Convert to integers.

y_pred_array = y_pred_array.astype(int)

np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:** 

We see in the above confusion matrices, Model 5 did an excellent job predicting a negative label when the tweet was negative but did not do as well with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance.

**Model 6: Bidirectional RNN.**

Recurrent Neural Network (RNN):

Unlike feedforward networks that process each input individually and independently, a RNN creates loops between each node in the neural network. This makes it particularly good for sequential data, such as text. It is able to process sequences and create a state which contains information about what the network has seen so far. This is why RNNs are useful for natural language processing, because sentences are decoded word-by-word while keeping memory of the words that came beforehand to give better context for understanding. A RNN allows information from a previous output to be fed as input into the current state. Simply put, we can use previous information to help make a current decision.

<br>

Bidirectional RNNs:

In general, RNNs tend to be order or time dependent. They process the time steps in a sequential, unidirectional order. On the other hand, a bidirectional RNN is able to process a sequence in both directions which means it may be able to pick up patterns that would not be noticed using a unidirectional model. Therefore, this type of model is able to improve performance on problems that have a chronological order.
"""

# Import the Tensorflow Biderectional RNN library.

from tensorflow.keras.layers import Bidirectional

# Bidirectional RNNs.

bdrnn_model_6 = Sequential()
bdrnn_model_6.add(embedding_layer)
bdrnn_model_6.add(Bidirectional(LSTM(64,
                              dropout=0.2,
                              recurrent_dropout=0.5)))
bdrnn_model_6.add(Dense(3,activation='softmax'))
bdrnn_model_6.summary()

bdrnn_model_6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# Tune the hyperparameters.

history_6 = bdrnn_model_6.fit(X_train, y_train,
                    validation_split = 0.2,
                    epochs=100, batch_size=256)

loss, accuracy = bdrnn_model_6.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = bdrnn_model_6.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

acc = history_6.history['acc']
val_acc = history_6.history['val_acc']
loss = history_6.history['loss']
val_loss = history_6.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**Observation:**

* Accuracy steadily increases to the point of overfitting in training but peaks at about 0.80 in validation.

* Loss decreases steadily in training but initially dips and then ascends to around 80%. Not ideal.
"""

# Get the predicted values.

y_pred = bdrnn_model_6.predict(X_test)  # Outputs probabilities of each sentiment.

# Create empty numpy array to match length of training observations.

y_pred_array = np.zeros(X_test.shape[0])

# Find the class with highest probability.

for i in range(0, y_pred.shape[0]):
    label_predict = np.argmax(y_pred[i]) # Column with max probability.
    y_pred_array[i] = label_predict

# Convert to integers.
y_pred_array = y_pred_array.astype(int)

np.set_printoptions(precision=2)

# Plot the non-normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot the normalized confusion matrix.

plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

"""**Observations:**

We see in the above confusion matrices, Model 6 did an excellent job predicting a negative label when the tweet was negative but did not do as well with predicting positive and neutral labels. This may be due to the fact that our training was largely comprised of negative tweets, so the model learned to give a higher probability to a negative label from this class imbalance.

# **Summary.**

* On Saturday February 22nd 2015 and Sunday February 23rd 2015, there was a bad weekend storm which dumped snow, sleet, and ice across the South and Eastern United States from Saturday into Sunday. At least 21 people died in Tennessee from storm-related fatalities, including hypothermia, as Gov. Bill Haslam (R) upgraded the state of emergency there to Level 2. Further north, Washington, D.C., and New York City each saw about five inches of snow, while snowed-in Boston received about another inch of powder. These events most likely had an impact on the types of tweets that we noticed peaked in that time frame.

<br>

* We see that in the positive word cloud, many positive words such as “love”, “amazing”, “thank”, and “fantastic” can be seen. This greatly contrasts the negative word cloud which has words such as “cancelled” and “delay”. The neutral world cloud seems to lack these emotionally polarizing words.

<br>

![image.png](data:image/png;base64,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)

<br>

* From our analysis, it seems that Reduced GRU With More Regularization is the best RNN to classify the sentiment of tweets. It does a better job at classifying both neutral and positive tweets compared to our other models. Therefore, it will better generalize to a different dataset that is less overwhelmingly comprised of negative tweets.

<br>

* More hyperparameter tuning and optimization is needed to better train the models. Training the neural networks for longer with more epochs and perhaps different batch sizes, learning rates, drop outs etc. may produce improved accuracy outcomes?

<br>

* Removing words like "flight" as a stopword may strengthen the sentiment analysis models because the word appears very frequently in all 3 classifications of the customers sentiment and most likely introduces "noise" into the dataset; it is neither a postive, negative, nor neutral word.

# **References.**

https://theweek.com/10things/536518/10-things-need-know-today-february22-2015

<br>

https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter12/.ipynb_checkpoints/Ch-12-checkpoint.ipynb

<br>

https://nlp.stanford.edu/projects/glove/

<br>

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

<br>

https://realpython.com/python-keras-text-classification/

<br>

https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/

<br>

https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

<br>

# **Extra:**
"""

df_Tweets_Stop = df_Tweets_Orig.copy()

df_Tweets_Stop = df_Tweets_Stop[["text","airline_sentiment"]]

#remove the html tags
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")                    
    return soup.get_text()

#expand the contractions
def replace_contractions(text):
    """Replace contractions in string of text"""
    return contractions.fix(text)

#remove the numericals present in the text
def remove_numbers(text):
  text = re.sub(r'\d+', '', text)
  return text

# remove the url's present in the text
def remove_url(text): 
    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)
    return text

# remove the mentions in the tweets
def remove_mention(text):
    text = re.sub(r'@\w+','',text)
    return text

def clean_text(text):
    text = strip_html(text)
    text = replace_contractions(text)
    text = remove_numbers(text)
    text = remove_url(text)
    text = remove_mention(text)
    return text
df_Tweets_Stop['text'] = df_Tweets_Stop['text'].apply(lambda x: clean_text(x))
df_Tweets_Stop.head(15)

df_Tweets_Stop['text'] = df_Tweets_Stop.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) # Tokenization of data
df_Tweets_Stop.head(15)

# This adds the word "flight" to the list of stopwords and effectively removes it from our 
# sentiment dataset for the purposes of model building.

stopwords.append('flight')  

lemmatizer = WordNetLemmatizer()

#remove the non-ASCII characters
def remove_non_ascii(words):
    """Remove non-ASCII characters from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

# convert all characters to lowercase
def to_lowercase(words):
    """Convert all characters to lowercase from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words


# Remove the hashtags
def remove_hash(text):
   """Remove hashtags from list of tokenized words"""
   new_words = []
   for word in words:
     new_word = re.sub(r'#\w+','',word)
     if new_word != '':
       new_words.append(new_word)
   return new_words

# Remove the punctuations
def remove_punctuation(words):
    """Remove punctuation from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', '', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words

# Remove the stop words
def remove_stopwords(words):
    """Remove stop words from list of tokenized words"""
    new_words = []
    for word in words:
        if word not in stopwords:
            new_words.append(word)
    return new_words

# lemmatize the words
def lemmatize_list(words):
    new_words = []
    for word in words:
      new_words.append(lemmatizer.lemmatize(word, pos='v'))
    return new_words

def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = remove_stopwords(words)
    words = lemmatize_list(words)
    return ' '.join(words)

df_Tweets_Stop['text'] = df_Tweets_Stop.apply(lambda row: normalize(row['text']), axis=1)

df_Tweets_Stop.head(15)

# importing all necessary modules 

from wordcloud import WordCloud
from wordcloud import STOPWORDS
import matplotlib.pyplot as plt
 
stopword_list = set(STOPWORDS) 

word_lists = df_Tweets_Stop['text']
unique_str  = ' '.join(word_lists)

#generate_wordcloud(unique_str)

word_cloud = WordCloud(width = 3000, height = 2500, 
                       background_color ='blue', 
                       stopwords = stopword_list, 
                       min_font_size = 10).generate(unique_str) 

# Visualize the WordCloud Plot

# Set wordcloud figure size

plt.figure(1,figsize=(12, 12)) 

# Show image

plt.imshow(word_cloud) 

# Remove Axis

plt.axis("off")  

# show plot

plt.show()

"""**Observation:**
<br>
The word "flight" is no longer present in the dataset.

**Feature Generation using CountVectorizer.**
"""

# Import CountVectorizer and RegexTokenizer
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import CountVectorizer

 
# Create Regex tokenizer for removing special symbols and numeric values 
regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
 
# Initialize CountVectorizer object
count_vectorizer = CountVectorizer(lowercase=True, 
                     stop_words='english', 
                     ngram_range = (1,1), 
                     tokenizer = regex_tokenizer.tokenize)
 
# Fit and transform the dataset
count_vectors = count_vectorizer.fit_transform(df_Tweets_Stop['text'])

"""**Split train and test.**"""

# Import train_test_split
from sklearn.model_selection import train_test_split
 
# Partition data into training and testing set 
from sklearn.model_selection import train_test_split
feature_train, feature_test, target_train, target_test = train_test_split(
    count_vectors, df_Tweets_Stop['airline_sentiment'], test_size=0.3, random_state=1)

"""**Classification Model Building using Logistic Regression.**"""

# import logistic regression scikit-learn model
from sklearn.linear_model import LogisticRegression
 
# instantiate the model
logreg = LogisticRegression(solver='lbfgs')
 
# fit the model with data
logreg.fit(feature_train,target_train)
 
# Forecast the target variable for given test dataset
predictions = logreg.predict(feature_test)

"""**Evaluate the Classification Model.**"""

# Import metrics module for performance evaluation
from sklearn.metrics import accuracy_score

# Assess model performance using accuracy measure
print("Logistic Regression Model Accuracy:",accuracy_score(target_test, predictions))

"""**Classification using TF-IDF.**"""

# Import TfidfVectorizer and RegexTokenizer
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Create Regex tokenizer for removing special symbols and numeric values 
regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
 
# Initialize TfidfVectorizer object
tfidf = TfidfVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = regex_tokenizer.tokenize)
 
# Fit and transform the dataset
text_tfidf= tfidf.fit_transform(df_Tweets_Stop['text'])

# Import train_test_split
from sklearn.model_selection import train_test_split
 
# Partition data into training and testing set 
from sklearn.model_selection import train_test_split
feature_train, feature_test, target_train, target_test = train_test_split(
    text_tfidf, df_Tweets_Stop['airline_sentiment'], test_size=0.3, random_state=1)

# import logistic regression scikit-learn model
from sklearn.linear_model import LogisticRegression
 
# instantiate the model
logreg = LogisticRegression(solver='lbfgs')
 
# fit the model with data
logreg.fit(feature_train,target_train)
 
# Forecast the target variable for given test dataset
predictions = logreg.predict(feature_test)

# Import metrics module for performance evaluation
from sklearn.metrics import accuracy_score

# Assess model performance using accuracy measure
print("Logistic Regression Model Accuracy:",accuracy_score(target_test, predictions))

"""**Observation:**

Removing the word "flight" did not improve the accuracy scores using both Logistic Regression and TF-IDF models. It remains approximately 77%.
"""